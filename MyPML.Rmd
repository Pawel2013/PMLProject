---
title: "PML - Practical Machine Learning Project from Coursera"
author: "Pawel Borowiec"
date: "Wednesday, October 22, 2014"
output: html_document
---

####Question:

The goal of this project is to build the predictor, which can answer the question:
What is the manner in which 6  participants are doing a particular activity ?.
The data this project uses are from accelerometers on the belt, forearm, arm, and dumbell.Participants performed  barbell lifts correctly and incorrectly in 5 different ways.The full description of the "Human Activity Recognition" experiment is given here: http://groupware.les.inf.puc-rio.br/har

####Input Data:

The training data comes from this source: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv.

The test data comes from this source: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.

####Features

After downloading and reading the training and test set I noticed, that there are 160 variables in both sets.
There is a column "new window" with "yes" or "no" values. For "yes" value in row -  statistical properties of some feature are given, but as we can see, the values are missed up. If I understand we don't need to mimmic original method applyied in author's paper - using rooling window, to calculate this statistical features (skewness, kurtosis, variance, etc.). Instead we must clean the data, because of lot of "NA" and "#DIV/0!" values and use almost the rest of columns. I suggest to remove first 8 columns.
As a result we have set of 52 features in both cleaned sets, plus "classe"" feature as outcome in training set.

```{r}
library(caret)
library(e1071)

# Read all training set
url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(url, "pml-training.csv")
trainingDS <- read.csv(file="pml-training.csv", header=TRUE, sep=",")
# remove all columns with NA or "#DIV/0!" 
trainingDSwithoutNA <- trainingDS[ , apply(trainingDS, 2, function(x) !any(x=="" || x=="#DIV/0!" || is.na(x)))]
# remove unnecessary columns
training <- trainingDSwithoutNA[-(1:7)]

# Read all test set
url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url, "pml-testing.csv")
testingDS <- read.csv(file="pml-testing.csv", header=TRUE, sep=",")
# remove all columns with NA or "#DIV/0!" or empty
testingDSwithoutNA <- testingDS[ , apply(testingDS, 2, function(x) !any(x=="" || x=="#DIV/0!" || is.na(x)))]
# remove unnecessary columns
test <- testingDSwithoutNA[-(1:7)]

# Find columns names for both tests
colnamesTrain <- colnames(training)
colnamesTest <- colnames(test)
# find the matching names
commonNames <- matchingNames <- names(training)[names(training) %in% names(test)]
# select only common names in training set
trainingNew <- training[,(colnamesTrain %in% commonNames)]
# select only common names in test set
testNew <- test[,(colnamesTest %in% commonNames)]
# add column "classe" to new training set
trainingNew["classe"] <- NA
trainingNew$classe <- trainingDS$classe

```

I devided cleaned training set into 2 parts as follows:
It was done for estimation of out-of-sample error for each model.

```{r}
# For Cross Validation
set.seed(1234)
inTrain <- createDataPartition(y = trainingNew$classe, p = 0.7, list = F)
trainingCV <- trainingNew[inTrain,]
# validation test
validatingCV <- trainingNew[-inTrain,]
```

Are these features correlated. To find it we can use....

```{r}
# Features selection & dimension reduction
#install.packages("FSelector")
library("FSelector")
result <- cfs(classe ~ ., trainingCV)
#result <- cfs(classe ~ ., trainNew)
f <- as.simple.formula(result, "classe")
print(f)
```

####Algorithm

I decided to use following 3 algorithms for comparison:
1)  Support Vector Machines
2)  Generalized Boosted Regression Models
3)  Random Forest

####Parameters of the model

For parameter estimation I used train function from caret package with:
10-fold cross validation and other default values.

```{r}
# Function for missclassification error estimation
missClass = function(values,prediction){sum(prediction != values)/length(values)}

# General function for model training
modelTrain <- function (m, f){
  if (m != "svm")
  {
    train(form=f, data=trainingCV, method=m, trControl=trainControl(method="cv",number=10)) 
  }
  else
  {
    svm(form=f, data=trainingCV, method=m, trControl=trainControl(method="cv",number=10))  
  }   
}
#
```
 
####Evaluation

All 3 models were tunned and evaluated using following code.Method (m value) was choosen to process current model ("svm", "gbm" and "rf")

##### Generalized Boosted Regression Models

```{r}
########## gbm

m = "gbm"
set.seed(1234)
timeStart <- Sys.time()
modFit <- modelTrain(m,f)
timeEnd <- Sys.time()
timeEnd-timeStart
modFit
modFit$finalModel

# in training prediction and confusion matrix
predictInTrainingCV <- predict(modFit, trainingCV)
confusionMatrix(predictInTrainingCV, trainingCV$classe)

# in testing prediction and confusion matrix
predictInValidatingCV <- predict(modFit, validatingCV)
confusionMatrix(predictInValidatingCV, validatingCV$classe)

# predict with validation set
prediction <- predict(modFit, validatingCV)
# real values
values <- validatingCV$classe
# missclassification error rate
gbmMissClassError <- missClass(values,prediction)

# predict with final test set
gbmPredictFinalTest <- predict(modFit, testNew)
gbmPredictFinalTest

##########
```

##### Support Vector Machines

```{r}
########## svm

m = "svm"
set.seed(1234)
timeStart <- Sys.time()
modFit <- modelTrain(m,f)
timeEnd <- Sys.time()
timeEnd-timeStart
modFit
modFit$finalModel

# in training prediction and confusion matrix
predictInTrainingCV <- predict(modFit, trainingCV)
confusionMatrix(predictInTrainingCV, trainingCV$classe)

# in testing prediction and confusion matrix
predictInValidatingCV <- predict(modFit, validatingCV)
confusionMatrix(predictInValidatingCV, validatingCV$classe)

# predict with validation set
prediction <- predict(modFit, validatingCV)
# real values
values <- validatingCV$classe
# missclassification error rate
svmMissClassError <- missClass(values,prediction)

# predict with final test set
svmPredictFinalTest <- predict(modFit, testNew)
svmPredictFinalTest

##########
```

##### Random Forest

```{r}
########## rf

m = "rf"
set.seed(1234)
timeStart <- Sys.time()
modFit <- modelTrain(m,f)
timeEnd <- Sys.time()
timeEnd-timeStart
modFit
modFit$finalModel

# in training prediction and confusion matrix
predictInTrainingCV <- predict(modFit, trainingCV)
confusionMatrix(predictInTrainingCV, trainingCV$classe)

# in testing prediction and confusion matrix
predictInValidatingCV <- predict(modFit, validatingCV)
confusionMatrix(predictInValidatingCV, validatingCV$classe)

# predict with validation set
prediction <- predict(modFit, validatingCV)
# real values
values <- validatingCV$classe
# missclassification error rate
rfMissClassError <- missClass(values,prediction)

# predict with final test set
rfPredictFinalTest <- predict(modFit, testNew)
rfPredictFinalTest

##########
```

####Rsults

As we can see, we get following estimation of out-of-sample error for 3 models:

for GBM

```{r}
gbmMissClassError
```

for SVM

```{r}
svmMissClassError
```

for RF

```{r}
rfMissClassError
```

and following predictions for 20-cases final test

for GBM

```{r}
gbmPredictFinalTest
```

for SVM

```{r}
svmPredictFinalTest
```

for RF

```{r}
rfPredictFinalTest
```

Model built with random forests is very accurate. In-sample accuraccy is equal 1.0, so we could expect overfitting, but accuracy for Ou-of-sample case is equal "only" 98,49% 

Applying RF model with the code:

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(rfPredictFinalTest)
```

we write each predicted case to seperate file, with "problem_id*" prefix.
Also worth mentioning is that in case as using all 52 selected features instead of given by csf package usage, we can expect even better accuracy.
